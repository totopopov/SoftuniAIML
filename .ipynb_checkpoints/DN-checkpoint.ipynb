{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Neural Networks 101</center></h1>\n",
    "<h2><center>RNN</center></h2>\n",
    "<p>The goal behind the current project is to be able to identify chemical names from common text with just splitting by space. A more specific direction in the goal is that the aim is towards the chemicals data set at the European Chemical Agency (ECHA). </p>\n",
    "\n",
    "<h2><center>Literature Review</center></h2>\n",
    "<p> In terms of data sets, on this http://cheminformatics.org/datasets/, there is sufficient amount of information regarding chemical compounds including their specific characteristics including inhibitor classifications. Another data set that has been designed for similar projects is CHEMDNER, a corpus of chemicals and drugs. The work I found on that data set is made on word embeddings and multiple classification analysis. However I wanted to test how would character embedding handle such a task. Nowadays most of the MLP processing is done with a form of RNN. Such is and this project. Similar projects like https://www.depends-on-the-definition.com/lstm-with-char-embeddings-for-ner/ rate at accuracy above 95%, and are deeper than expected initially.    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Data preparation and augmentation</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "def write_json_file(name,data):\n",
    "    with open(name+'.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def read_json_file(name):\n",
    "    with open(name+'.json', 'r', encoding='utf-8-sig') as file:\n",
    "        read_data = json.load(file)\n",
    "    return read_data\n",
    "\n",
    "def read_csv(name):\n",
    "    with open(name + '.csv', 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        result = [i for i in spamreader]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aquaire_data_from_iuclid():\n",
    "    #In order to obtain the data you need a iuclid server or in my case a local uiclid server\n",
    "\n",
    "    responce = requests.get('http://localhost:8080/iuclid6-ext/api/ext/v1/inventory?l=300000')\n",
    "    data=[]\n",
    "    for i in responce.json()['results']:\n",
    "        if 'name' in i['representation']:\n",
    "            data.append(i['representation']['name'])\n",
    "    write_json_file('iuclid_row',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Prepare the data</center></h1>\n",
    "\n",
    "<p>As mentioned before, the target chemical names are in the EUCH data base. In order to get access to their chemicals, one needs to go through their local/server application https://iuclid6.echa.europa.eu/download in order to parse the available data for chemicals that can be found here https://iuclid6.echa.europa.eu/inventories-iuclid. In addition the context data set to be used in this project was taken from the previously mentioned CHEMDNER. What we have in the end is a mix between 150 000 chemical names and around 600 000 words and text symbols to produce noise. Both data sets are cured of unwanted utf characters like hieroglyphs /less than 10 words are taken out./</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_json_file('iuclid_row')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As some of the chemical names consist of 2 words, however in the current flow we look at only one word to classify it. Therefore all combinations of words are slitted and only the unique are taken. The reason behind this is that if 2 chemical names are identified one after the other then there is a high chance they belong together.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_frequency={}\n",
    "for name in data:\n",
    "    tokens = name.lower().split()\n",
    "    for token in tokens:\n",
    "        token_frequency[token] = token_frequency.get(token,0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Extracting some words by hand from the result set and also getting familiar with the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(token_frequency, orient='index', columns=['frequency'])\n",
    "df.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "drop_words = ['and','of','reaction','mass','with','products','6','(1:2)','/','<2%','by',\n",
    "              'orange','the', 'cracked','lights','from','tall','gum','cas',',','a']\n",
    "df.drop(drop_words,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extracted_key_words = []\n",
    "for index, row in df.iterrows():\n",
    "    if u'\\uF061' in index or u'\\uf061' in index or u'\\uf06b' in index or u'\\u200b' in index:\n",
    "        continue\n",
    "    extracted_key_words.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_tokens=set()\n",
    "\n",
    "greek_letters=[\n",
    "    u'\\u03B1',\n",
    "    u'\\u03B2',\n",
    "    u'\\u03B3',\n",
    "    u'\\u03B4',\n",
    "    u'\\u03B5',\n",
    "    u'\\u03B6',\n",
    "    u'\\u03B7',\n",
    "    u'\\u03B8',\n",
    "    u'\\u03B9',\n",
    "    u'\\u03BA',\n",
    "    u'\\u03BB',\n",
    "    u'\\u03BC',\n",
    "    u'\\u03BD',\n",
    "    u'\\u03BE',\n",
    "    u'\\u03BF',\n",
    "    u'\\u03C0',\n",
    "    u'\\u03C1',\n",
    "    u'\\u03C2',\n",
    "    u'\\u03C3',\n",
    "    u'\\u03C4',\n",
    "    u'\\u03C5',\n",
    "    u'\\u03C6',\n",
    "    u'\\u03C7',\n",
    "    u'\\u03C8',\n",
    "    u'\\u03C9']\n",
    "\n",
    "hot_tokens.update(greek_letters)\n",
    "hot_tokens.add('â‰¤')\n",
    "\n",
    "\n",
    "for word in extracted_key_words:\n",
    "    for token in word:\n",
    "        if token == u'\\uF061' or token == u'\\uf061' or token == u'\\u200b':\n",
    "            print(word)\n",
    "        hot_tokens.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words_to_tokens(extracted_key_words,hot_tokens):\n",
    "    cheker=len(hot_tokens)\n",
    "    for word in extracted_key_words:\n",
    "        for token in word:\n",
    "            if token == u'\\uF061' or token == u'\\uf061' or token == u'\\u200b':\n",
    "                print(word)\n",
    "            hot_tokens.add(token)\n",
    "    return False if len(hot_tokens)==cheker else len(hot_tokens)-cheker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens= read_csv('text')\n",
    "text_tokens.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_fill_tokens = {}\n",
    "\n",
    "for token in text_tokens:\n",
    "    distribution_fill_tokens[token[3]] = distribution_fill_tokens.get(token[3],0)+1\n",
    "\n",
    "distribution_fill_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Taken only data classified as ordinary from CHEMDNER</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = [i[2].lower() for i in text_tokens if (i[3]=='O' and u'\\xa0' not in i[2].lower())]\n",
    "add_words_to_tokens(text_tokens,hot_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> A Monte Carlo black-jack is used to keep distribution and sparsity of data as expected to be found. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from random import random\n",
    "\n",
    "balance = len(extracted_key_words)*1.0/(len(text_tokens)+len(extracted_key_words))\n",
    "print(balance)\n",
    "building_corpus=[]\n",
    "while True:\n",
    "    black_jack=(random())\n",
    "    if black_jack>balance:\n",
    "        index=randrange(len(text_tokens))\n",
    "        building_corpus.append([text_tokens.pop(index),0])\n",
    "    else:\n",
    "        index=randrange(len(extracted_key_words))\n",
    "        building_corpus.append([extracted_key_words.pop(index),1])\n",
    "    if not (text_tokens and extracted_key_words):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_file('corpus',building_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Encode Data</center></h1>\n",
    "\n",
    "<P> Initially I was going for a one_hot_encoding but then I saw that an embedding layer can also do the trick so I decided to go with it. Based on https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12 and the fact that the size of the data is quite big I decided to switch to a embedding layer.<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "identity=np.identity(len(hot_tokens))\n",
    "\n",
    "hot_tokens_encoded={i:list(identity[k]) for k,i in enumerate(hot_tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_file('encoding',hot_tokens_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_tokens=[i for i in hot_tokens]\n",
    "hot_tokens.insert(0,' ')\n",
    "for token in building_corpus:\n",
    "    to_be_encoded=token[0]\n",
    "    encoding_matrix=[]\n",
    "    for charachter in to_be_encoded:\n",
    "        encoding_matrix.append(hot_tokens.index(charachter))  \n",
    "    token[0]=encoding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_file('encoded_corpus',building_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready_for_model={}\n",
    "data_ready_for_model['corpus']=building_corpus\n",
    "data_ready_for_model['mapping']=hot_tokens\n",
    "write_json_file('data_for_model',data_ready_for_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>One can skip all of the above if you just load the initial cells with functions and then go directly from here.<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready_for_model=read_json_file('data_for_model')\n",
    "building_corpus=data_ready_for_model['corpus']\n",
    "hot_tokens=data_ready_for_model['mapping']\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.606791610523594\n",
      "411\n"
     ]
    }
   ],
   "source": [
    "num_tokens = [len(tokens[0]) for tokens in building_corpus]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print(np.mean(num_tokens))\n",
    "print(np.max(num_tokens))\n",
    "max_tokens=np.max(num_tokens)\n",
    "x_data=[i[0] for i in building_corpus]\n",
    "y_data=[i[1] for i in building_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I decision was taken to not trim any data, because the chemical names were the once that were lifting the max(num_tokens) so high.<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding,CuDNNLSTM,LSTM,Dropout\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau,Callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> When I was using the GRU layers, the GPU was handling them at 7 slower than the CPU, therefore big, big attention, GPU is not always the fastest. <\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad='post'\n",
    "x_data_pad = pad_sequences(x_data, maxlen=max_tokens,\n",
    "                            padding=pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "734730"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_pad.shape\n",
    "\n",
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_data_pad[:720000]\n",
    "y_train=y_data[:720000]\n",
    "x_test=x_data_pad[720000:]\n",
    "y_test=y_data[720000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36000.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "720000*0.05  # calculating validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Building the Model</center></h1>\n",
    "\n",
    "<p>Initially I started not with a very simple model (Contrary to the given initial directions) I came across this https://arxiv.org/pdf/1412.3555v1.pdf paper and decided to test it with GRU. Also it looks like a transistor /I thought its worth it a shot/.<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 411, 170)          28900     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 411, 16)           8976      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 411, 8)            600       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 38,637\n",
      "Trainable params: 38,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = len(hot_tokens)\n",
    "model.add(Embedding(input_dim=embedding_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens))\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 411, 170)          28900     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       (None, 411, 128)          153600    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 411, 128)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 32)                20736     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,269\n",
      "Trainable params: 203,269\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = len(hot_tokens)\n",
    "model.add(Embedding(input_dim=embedding_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens))\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(32))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# add decay                     \n",
    "optimizer = Adam(lr=1e-3,decay=1e-6)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After the initial run up with GRU, it seemed that the model has a bias, and it seemed not to be able to fit the model after certain treshhold. Therefore I constructed a similar model with LSTM this time, they were running better on GPU. After running the same results appeared, so I added a decay in the optimiser with the intention to make the learning rate smaller and to be able to fit the model a little bit better. The success could be state to be moderate. <\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A very nice extension of a Callback function to represent acc after every epoch. Of course accuracy needs to be looked at after the complete training, but this additional information kind of gives the notion of where the model is going. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_checkpoint='Project1_v1.2'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      # save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=5, verbose=1)\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='./23_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=True)\n",
    "test_callback=TestCallback((x_test, y_test))\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 684000 samples, validate on 36000 samples\n",
      "Epoch 1/2\n",
      "683968/684000 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8595\n",
      "Epoch 00001: val_loss did not improve from 0.37084\n",
      "684000/684000 [==============================] - 1321s 2ms/step - loss: 0.4062 - acc: 0.8595 - val_loss: 0.4072 - val_acc: 0.8588\n",
      "Epoch 2/2\n",
      "683968/684000 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8595\n",
      "Epoch 00002: val_loss did not improve from 0.37084\n",
      "684000/684000 [==============================] - 1360s 2ms/step - loss: 0.4061 - acc: 0.8595 - val_loss: 0.4072 - val_acc: 0.8588\n",
      "Wall time: 44min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x66ca37db70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train, y_train,\n",
    "          validation_split=0.05, epochs=2, batch_size=64,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('path_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Conclusion</center></h1>\n",
    "\n",
    "<p>For the state to where the current model development is, there are several steps that can be taken in order to improve performance. First is to test it with more than 10 epoch to see if at some moment the network wont find revelations. Another would be to reduce the drop outs and see if fitting the model is going to improve. Third option would be to check with a RMS optimiser instead of adam, because in some of the RNN, RMS seemed to be working better. Overall the goal of identifying chemical names was achieved to a satisfying degree with potential to be developed further.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aiml]",
   "language": "python",
   "name": "conda-env-aiml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
